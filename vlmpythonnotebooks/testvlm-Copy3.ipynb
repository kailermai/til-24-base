{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c76199-0761-4c2f-af53-dde67b1bed5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 143\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# def show_label_batch(sample_batched):\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#     images_batch, labels_batch = sample_batched[\"image\"], sample_batched[\"label\"]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#     batch_size = len(images_batch)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#Image Loading]\u001b[39;00m\n\u001b[1;32m    141\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(our_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m--> 143\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# print(f\"Feature batch shape: {train_features.size()}\")\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# print(f\"Labels batch shape: {train_labels.size()}\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m img \u001b[38;5;241m=\u001b[39m train_features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]):\n\u001b[1;32m     87\u001b[0m     tensors, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 88\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(targets)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features, targets\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/utils/rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    395\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creating a Dataset\n",
    "\n",
    "dataPath = \"/home/jupyter/novice\"\n",
    "labels_file = os.path.join(dataPath, \"vlm.jsonl\")\n",
    "images_folder = os.path.join(dataPath, \"images\")\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, labels_file, images_folder, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_json(labels_file, lines=True)\n",
    "        self.img_dir = images_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        label = np.array([label])\n",
    "        sample = {\"image\" : image, \"label\" : label}\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return sample\n",
    "    \n",
    "our_dataset = CustomImageDataset(labels_file, images_folder)\n",
    "\n",
    "\n",
    "#Implementing SSD300 VGG16 model\n",
    "\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "ssd_model = torchvision.models.detection.ssd300_vgg16(\n",
    "    weights=True, box_score_thresh=0.9\n",
    ")\n",
    "ssd_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "#Collate_FN\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     _, labels, lengths = zip(*data)\n",
    "#     max_len = max(lengths)\n",
    "#     n_ftrs = data[0][0].size(1)\n",
    "#     features = torch.zeros((len(data), max_len, n_ftrs))\n",
    "#     labels = torch.tensor(labels)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "    \n",
    "#     for i in range(len(data)):\n",
    "#         j, k = data[i][0].size(0), data([i][o].size(1))\n",
    "#         features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
    "#     return features.float(), labels.long(), lengths.long()\n",
    "\n",
    "def collate_fn(data: list[tuple[torch.Tensor, torch.Tensor]]):\n",
    "    tensors, targets = zip(*data)\n",
    "    features = pad_sequence(tensors, batch_first=True)\n",
    "    targets = torch.stack(targets)\n",
    "    return features, targets\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(our_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# def show_label_batch(sample_batched):\n",
    "#     images_batch, labels_batch = sample_batched[\"image\"], sample_batched[\"label\"]\n",
    "#     batch_size = len(images_batch)\n",
    "#     im_size = images_batch.size(2)\n",
    "    \n",
    "# for i_batch, sample_batched in enumerate(dataloader):\n",
    "#     print(i_batch, sample_batched[\"image\"].size(),\n",
    "#           sample_batched[\"label\"].size())\n",
    "    \n",
    "#     if i_batch == 3:\n",
    "#         plt.figure()\n",
    "#         show_label_batch(sample_batched)\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "#         break\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Printing Image and Label\n",
    "# fig = plt.figure()\n",
    "\n",
    "# for i, sample in enumerate(our_dataset):\n",
    "#     print(i, sample[\"image\"].shape, sample[\"label\"].shape)\n",
    "#     print(sample[\"label\"])\n",
    "#     ax = plt.subplot(1,4,i+1)\n",
    "#     plt.tight_layout()\n",
    "#     ax.axis('off')\n",
    "#     plt.imshow(sample[\"image\"].permute(1,2,0))\n",
    "#     plt.show()\n",
    "    \n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "#Image Loading]\n",
    "\n",
    "train_dataloader = DataLoader(our_dataset, batch_size = 4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "##figure out padding and loading multiple images\n",
    "##transfer learning and fine tuning \n",
    "##test output before training (can it detect airplane first)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b54c48-fdec-4ea4-8e8d-dac3a57d0e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
