{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c76199-0761-4c2f-af53-dde67b1bed5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#Image Loading]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(our_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[0;32m---> 69\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print(f\"Feature batch shape: {train_features.size()}\")\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(f\"Labels batch shape: {train_labels.size()}\")\u001b[39;00m\n\u001b[1;32m     72\u001b[0m img \u001b[38;5;241m=\u001b[39m train_features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creating a Dataset\n",
    "\n",
    "dataPath = \"/home/jupyter/novice\"\n",
    "labels_file = os.path.join(dataPath, \"vlm.jsonl\")\n",
    "images_folder = os.path.join(dataPath, \"images\")\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, labels_file, images_folder, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_json(labels_file, lines=True)\n",
    "        self.img_dir = images_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "our_dataset = CustomImageDataset(labels_file, images_folder)\n",
    "\n",
    "\n",
    "#Implementing SSD300 VGG16 model\n",
    "\n",
    "weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "ssd_model = torchvision.models.detection.ssd300_vgg16(\n",
    "    weights=True, box_score_thresh=0.9\n",
    ")\n",
    "ssd_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "#Image Loading]\n",
    "\n",
    "train_dataloader = DataLoader(our_dataset, batch_size = 4, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "##figure out padding and loading multiple images\n",
    "##transfer learning and fine tuning \n",
    "##test output before training (can it detect airplane first)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b54c48-fdec-4ea4-8e8d-dac3a57d0e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[[250, 250, 250,  ...,  26,  27,  26],\n",
       "           [250, 250, 250,  ...,  25,  26,  26],\n",
       "           [249, 249, 250,  ...,  22,  23,  22],\n",
       "           ...,\n",
       "           [188, 187, 188,  ...,  78,  75,  72],\n",
       "           [170, 165, 162,  ...,  77,  76,  74],\n",
       "           [134, 125, 122,  ...,  75,  75,  74]],\n",
       "  \n",
       "          [[252, 252, 252,  ...,  26,  24,  23],\n",
       "           [252, 252, 252,  ...,  25,  23,  23],\n",
       "           [251, 251, 252,  ...,  23,  23,  22],\n",
       "           ...,\n",
       "           [170, 169, 168,  ...,  71,  68,  65],\n",
       "           [151, 146, 143,  ...,  70,  69,  67],\n",
       "           [115, 106, 103,  ...,  68,  68,  67]],\n",
       "  \n",
       "          [[251, 251, 251,  ...,  16,  15,  14],\n",
       "           [251, 251, 251,  ...,  15,  14,  14],\n",
       "           [250, 250, 251,  ...,  15,  15,  14],\n",
       "           ...,\n",
       "           [132, 131, 133,  ...,  55,  50,  47],\n",
       "           [118, 113, 111,  ...,  52,  51,  49],\n",
       "           [ 83,  74,  73,  ...,  50,  50,  49]]], dtype=torch.uint8),\n",
       "  [{'caption': 'blue and white missile', 'bbox': [1224, 284, 44, 36]},\n",
       "   {'caption': 'green light aircraft', 'bbox': [688, 400, 56, 36]},\n",
       "   {'caption': 'blue and white commercial aircraft',\n",
       "    'bbox': [800, 320, 128, 36]}]),\n",
       " (tensor([[[135, 135, 134,  ..., 182, 180, 179],\n",
       "           [136, 135, 135,  ..., 181, 180, 179],\n",
       "           [136, 136, 136,  ..., 180, 179, 178],\n",
       "           ...,\n",
       "           [173, 164, 148,  ...,  32,  31,  30],\n",
       "           [152, 139, 127,  ...,  38,  39,  40],\n",
       "           [140, 133, 131,  ...,  41,  46,  48]],\n",
       "  \n",
       "          [[144, 144, 143,  ..., 187, 186, 185],\n",
       "           [145, 144, 144,  ..., 187, 186, 185],\n",
       "           [145, 145, 145,  ..., 186, 185, 186],\n",
       "           ...,\n",
       "           [172, 163, 147,  ...,  32,  31,  31],\n",
       "           [151, 138, 126,  ...,  39,  40,  41],\n",
       "           [139, 132, 130,  ...,  42,  47,  49]],\n",
       "  \n",
       "          [[177, 177, 176,  ..., 209, 208, 207],\n",
       "           [178, 177, 177,  ..., 209, 208, 207],\n",
       "           [178, 178, 178,  ..., 208, 209, 209],\n",
       "           ...,\n",
       "           [151, 142, 126,  ...,  30,  29,  26],\n",
       "           [130, 117, 105,  ...,  34,  35,  35],\n",
       "           [118, 111, 109,  ...,  37,  41,  43]]], dtype=torch.uint8),\n",
       "  [{'caption': 'blue commercial aircraft', 'bbox': [1156, 496, 104, 60]},\n",
       "   {'caption': 'white and yellow commercial aircraft',\n",
       "    'bbox': [1296, 488, 136, 44]}]),\n",
       " (tensor([[[118, 117, 116,  ..., 201, 203, 206],\n",
       "           [117, 117, 116,  ..., 206, 208, 212],\n",
       "           [116, 116, 116,  ..., 208, 212, 215],\n",
       "           ...,\n",
       "           [ 47,  47,  47,  ...,  50,  50,  50],\n",
       "           [ 47,  47,  47,  ...,  49,  49,  49],\n",
       "           [ 46,  47,  47,  ...,  49,  48,  48]],\n",
       "  \n",
       "          [[160, 159, 158,  ..., 178, 181, 184],\n",
       "           [159, 159, 158,  ..., 183, 186, 190],\n",
       "           [158, 158, 158,  ..., 185, 189, 192],\n",
       "           ...,\n",
       "           [ 61,  61,  61,  ...,  64,  64,  64],\n",
       "           [ 61,  61,  61,  ...,  63,  63,  63],\n",
       "           [ 60,  61,  61,  ...,  63,  62,  62]],\n",
       "  \n",
       "          [[208, 207, 206,  ..., 137, 142, 145],\n",
       "           [207, 207, 206,  ..., 142, 147, 151],\n",
       "           [206, 206, 206,  ..., 144, 148, 151],\n",
       "           ...,\n",
       "           [ 72,  72,  72,  ...,  77,  77,  77],\n",
       "           [ 72,  72,  72,  ...,  76,  76,  76],\n",
       "           [ 71,  72,  72,  ...,  76,  75,  75]]], dtype=torch.uint8),\n",
       "  [{'caption': 'white and blue fighter jet', 'bbox': [488, 196, 52, 44]},\n",
       "   {'caption': 'blue and yellow fighter jet', 'bbox': [836, 464, 36, 36]},\n",
       "   {'caption': 'grey and white fighter plane', 'bbox': [1060, 208, 64, 32]}]),\n",
       " (tensor([[[157, 157, 157,  ..., 238, 237, 237],\n",
       "           [157, 157, 158,  ..., 237, 237, 236],\n",
       "           [158, 158, 156,  ..., 236, 236, 236],\n",
       "           ...,\n",
       "           [134, 134, 135,  ..., 131, 132, 133],\n",
       "           [131, 131, 132,  ..., 129, 128, 128],\n",
       "           [129, 129, 130,  ..., 129, 128, 126]],\n",
       "  \n",
       "          [[165, 165, 165,  ..., 235, 234, 234],\n",
       "           [165, 165, 166,  ..., 234, 234, 233],\n",
       "           [166, 166, 167,  ..., 233, 233, 233],\n",
       "           ...,\n",
       "           [152, 152, 151,  ..., 148, 149, 150],\n",
       "           [149, 149, 148,  ..., 146, 145, 145],\n",
       "           [147, 147, 146,  ..., 146, 145, 143]],\n",
       "  \n",
       "          [[184, 184, 184,  ..., 228, 229, 229],\n",
       "           [184, 184, 185,  ..., 227, 229, 228],\n",
       "           [185, 185, 185,  ..., 228, 228, 228],\n",
       "           ...,\n",
       "           [166, 166, 166,  ..., 168, 167, 168],\n",
       "           [163, 163, 163,  ..., 166, 163, 163],\n",
       "           [161, 161, 161,  ..., 166, 163, 161]]], dtype=torch.uint8),\n",
       "  [{'caption': 'grey camouflage fighter jet', 'bbox': [212, 444, 72, 64]},\n",
       "   {'caption': 'grey and black helicopter', 'bbox': [912, 144, 40, 28]},\n",
       "   {'caption': 'grey commercial aircraft', 'bbox': [536, 116, 72, 52]}])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca392f-436d-40f0-885c-40741d32e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
